name: Deploy Strapi to AWS ECS


on:
  push:
    branches:
      - main

env:
  AWS_REGION: us-east-1
  TERRAFORM_DIR: terraform
  TF_STATE_BUCKET: strapi-terraform-state
  TF_STATE_KEY: strapi/state/terraform.tfstate
  TF_DYNAMODB_TABLE: strapi-terraform-lock

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
      
      - name: Check commit message
        id: check-commit
        run: |
          if [[ "${{ github.event.head_commit.message }}" =~ "destroy" ]]; then
            echo "DESTROY=true" >> $GITHUB_ENV
          else
            echo "DESTROY=false" >> $GITHUB_ENV
          fi
        

      - name: Debug directory structure
        run: |
          echo "Listing repository root contents:"
          ls -la
          echo "Current working directory:"
          pwd
          echo "Listing terraform directory contents (if exists):"
          ls -la ${{ env.TERRAFORM_DIR }} || echo "No terraform directory found"
          echo "Checking for Terraform configuration files:"
          if [ -f main.tf ]; then
            echo "Found main.tf in root"
          elif [ -f ${{ env.TERRAFORM_DIR }}/main.tf ]; then
            echo "Found main.tf in terraform directory"
          else
            echo "Error: No main.tf found in root or terraform directory"
            echo "Creating minimal main.tf in terraform directory as fallback"
            mkdir -p ${{ env.TERRAFORM_DIR }}
            echo "# Minimal Terraform configuration\nterraform {\n  required_providers {\n    aws = { source = \"hashicorp/aws\", version = \"~> 4.0\" }\n  }\n  backend \"s3\" {\n    bucket         = \"${{ env.TF_STATE_BUCKET }}\"\n    key            = \"${{ env.TF_STATE_KEY }}\"\n    region         = \"${{ env.AWS_REGION }}\"\n    dynamodb_table = \"${{ env.TF_DYNAMODB_TABLE }}\"\n  }\n}\nprovider \"aws\" {\n  region = \"${{ env.AWS_REGION }}\"\n}" > ${{ env.TERRAFORM_DIR }}/main.tf
          fi
          echo "Checking for modules directory:"
          if [ -d modules ] || [ -d ${{ env.TERRAFORM_DIR }}/modules ]; then
            echo "Modules directory found"
          else
            echo "Warning: No modules directory found in root or terraform directory"
            mkdir -p ${{ env.TERRAFORM_DIR }}/modules
            echo "Created placeholder modules directory to prevent errors"
          fi

      - name: Set up Terraform directory
        run: |
          echo "Setting up Terraform directory"
          mkdir -p ${{ env.TERRAFORM_DIR }}
          if [ -f main.tf ] && [ ! -f ${{ env.TERRAFORM_DIR }}/main.tf ]; then
            mv *.tf ${{ env.TERRAFORM_DIR }}/
            if [ -d modules ]; then
              mv modules ${{ env.TERRAFORM_DIR }}/
            fi
          fi
          echo "Terraform directory contents after setup:"
          ls -la ${{ env.TERRAFORM_DIR }} || echo "Terraform directory is empty"
          if [ -f ${{ env.TERRAFORM_DIR }}/main.tf ]; then
            echo "Contents of main.tf:"
            cat ${{ env.TERRAFORM_DIR }}/main.tf
          else
            echo "Error: main.tf still not found in terraform directory"
            exit 1
          fi
          echo "Checking for local terraform.tfstate (for reference):"
          if [ -f ${{ env.TERRAFORM_DIR }}/terraform.tfstate ]; then
            echo "Found local terraform.tfstate"
            cat ${{ env.TERRAFORM_DIR }}/terraform.tfstate
          else
            echo "No local terraform.tfstate found (expected with S3 backend)"
          fi

      - name: Set Terraform working directory
        id: set-working-dir
        run: |
          if [ -f ${{ env.TERRAFORM_DIR }}/main.tf ]; then
            echo "TERRAFORM_WORKING_DIR=${{ env.TERRAFORM_DIR }}" >> $GITHUB_ENV
          else
            echo "TERRAFORM_WORKING_DIR=." >> $GITHUB_ENV
          fi

      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
     

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.5.0

      - name: Terraform Init
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: terraform init -backend-config="bucket=${{ env.TF_STATE_BUCKET }}" -backend-config="key=${{ env.TF_STATE_KEY }}" -backend-config="region=${{ env.AWS_REGION }}" -backend-config="dynamodb_table=${{ env.TF_DYNAMODB_TABLE }}"
        continue-on-error: false

      - name: Check existing resources
        if: env.DESTROY == 'true'
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: |
          echo "Checking for existing resources in AWS:"
          aws ecs list-clusters --region ${{ env.AWS_REGION }} || echo "No ECS clusters found"
          aws rds describe-db-clusters --region ${{ env.AWS_REGION }} || echo "No RDS clusters found"
          aws elbv2 describe-load-balancers --region ${{ env.AWS_REGION }} || echo "No ALBs found"
          aws ecr describe-repositories --region ${{ env.AWS_REGION }} || echo "No ECR repositories found"
          aws route53 list-hosted-zones || echo "No Route53 hosted zones found"
          aws iam get-user --user-name github-actions-user  || echo "No IAM user 'github-actions-user ' found"
          aws iam get-user-policy --user-name github-actions-user  --policy-name github-actions-policy || echo "No IAM policy 'github-actions-policy' found for user"

      - name: Clean up dependencies
        if: env.DESTROY == 'true'
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: |
          echo "Cleaning up dependencies before Terraform Destroy"
          # Scale down ECS services to zero
          if aws ecs list-services --cluster strapi-cluster --region ${{ env.AWS_REGION }} | grep -q 'strapi-prod-service'; then
            aws ecs update-service --cluster strapi-cluster --service strapi-prod-service --desired-count 0 --region ${{ env.AWS_REGION }}
            aws ecs delete-service --cluster strapi-cluster --service strapi-prod-service --region ${{ env.AWS_REGION }}
          else
            echo "No ECS service 'strapi-prod-service' found"
          fi
          if aws ecs list-services --cluster strapi-cluster --region ${{ env.AWS_REGION }} | grep -q 'strapi-staging-service'; then
            aws ecs update-service --cluster strapi-cluster --service strapi-staging-service --desired-count 0 --region ${{ env.AWS_REGION }}
            aws ecs delete-service --cluster strapi-cluster --service strapi-staging-service --region ${{ env.AWS_REGION }}
          else
            echo "No ECS service 'strapi-staging-service' found"
          fi
          # Delete ECR images
          if aws ecr describe-repositories --repository-names strapi-app --region ${{ env.AWS_REGION }} | grep -q '"repositoryName": "strapi-app"'; then
            IMAGE_DIGESTS=$(aws ecr list-images --repository-name strapi-app --region ${{ env.AWS_REGION }} --query 'imageIds[].imageDigest' --output text)
            if [ -n "$IMAGE_DIGESTS" ]; then
              for DIGEST in $IMAGE_DIGESTS; do
                aws ecr batch-delete-image --repository-name strapi-app --image-ids imageDigest=$DIGEST --region ${{ env.AWS_REGION }}
              done
            else
              echo "No images found in ECR repository 'strapi-app'"
            fi
          else
            echo "No ECR repository 'strapi-app' found"
          fi
          # Delete IAM access keys
          if aws iam get-user --user-name github-actions-user | grep -q '"UserName": "github-actions-user"'; then
            ACCESS_KEYS=$(aws iam list-access-keys --user-name github-actions-user --query 'AccessKeyMetadata[].AccessKeyId' --output text)
            if [ -n "$ACCESS_KEYS" ]; then
              for KEY in $ACCESS_KEYS; do
                aws iam delete-access-key --user-name github-actions-user --access-key-id $KEY
              done
            else
              echo "No access keys found for IAM user 'github-actions-user'"
            fi
          else
            echo "No IAM user 'github-actions-user' found"
          fi
          # Clean up VPC dependencies
          if aws ec2 describe-vpcs --filters Name=tag:Name,Values=strapi-vpc --region ${{ env.AWS_REGION }} | grep -q '"VpcId"'; then
            VPC_ID=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values=strapi-vpc --region ${{ env.AWS_REGION }} --query 'Vpcs[0].VpcId' --output text)
            if [ -n "$VPC_ID" ]; then
              # Disassociate and release Elastic IPs
              EIPS=$(aws ec2 describe-addresses --filters Name=domain,Values=vpc --region ${{ env.AWS_REGION }} --query 'Addresses[].AllocationId' --output text)
              if [ -n "$EIPS" ]; then
                for EIP in $EIPS; do
                  ASSOCIATION_ID=$(aws ec2 describe-addresses --allocation-ids $EIP --region ${{ env.AWS_REGION }} --query 'Addresses[0].AssociationId' --output text)
                  if [ -n "$ASSOCIATION_ID" ] && [ "$ASSOCIATION_ID" != "None" ]; then
                    aws ec2 disassociate-address --association-id $ASSOCIATION_ID --region ${{ env.AWS_REGION }} || echo "Failed to disassociate EIP $EIP"
                  fi
                  aws ec2 release-address --allocation-id $EIP --region ${{ env.AWS_REGION }} || echo "Failed to release EIP $EIP"
                done
              else
                echo "No Elastic IPs found in VPC $VPC_ID"
              fi
              # Delete NAT Gateways
              NAT_GATEWAYS=$(aws ec2 describe-nat-gateways --filter Name=vpc-id,Values=$VPC_ID --region ${{ env.AWS_REGION }} --query 'NatGateways[].NatGatewayId' --output text)
              if [ -n "$NAT_GATEWAYS" ]; then
                for NAT_GW in $NAT_GATEWAYS; do
                  aws ec2 delete-nat-gateway --nat-gateway-id $NAT_GW --region ${{ env.AWS_REGION }} || echo "Failed to delete NAT Gateway $NAT_GW"
                  echo "Waiting for NAT Gateway $NAT_GW to be deleted"
                  aws ec2 wait nat-gateway-deleted --nat-gateway-ids $NAT_GW --region ${{ env.AWS_REGION }}
                done
              else
                echo "No NAT Gateways found in VPC $VPC_ID"
              fi
            if [ -n "$VPC_ID" ]; then
              # Delete subnets
              SUBNETS=$(aws ec2 describe-subnets --filters Name=vpc-id,Values=$VPC_ID --region ${{ env.AWS_REGION }} --query 'Subnets[].SubnetId' --output text)
              for SUBNET in $SUBNETS; do
                aws ec2 delete-subnet --subnet-id $SUBNET --region ${{ env.AWS_REGION }} || echo "Failed to delete subnet $SUBNET"
              done
              # Delete route tables (except main)
              ROUTE_TABLES=$(aws ec2 describe-route-tables --filters Name=vpc-id,Values=$VPC_ID --region ${{ env.AWS_REGION }} --query 'RouteTables[?Associations[?Main!=`true`]].RouteTableId' --output text)
              for RT in $ROUTE_TABLES; do
                aws ec2 delete-route-table --route-table-id $RT --region ${{ env.AWS_REGION }} || echo "Failed to delete route table $RT"
              done
              # Delete internet gateway
              IGW=$(aws ec2 describe-internet-gateways --filters Name=attachment.vpc-id,Values=$VPC_ID --region ${{ env.AWS_REGION }} --query 'InternetGateways[].InternetGatewayId' --output text)
              if [ -n "$IGW" ]; then
                aws ec2 detach-internet-gateway --internet-gateway-id $IGW --vpc-id $VPC_ID --region ${{ env.AWS_REGION }}
                aws ec2 delete-internet-gateway --internet-gateway-id $IGW --region ${{ env.AWS_REGION }} || echo "Failed to delete internet gateway $IGW"
              fi
              # Delete security groups (except default)
              SECURITY_GROUPS=$(aws ec2 describe-security-groups --filters Name=vpc-id,Values=$VPC_ID --region ${{ env.AWS_REGION }} --query 'SecurityGroups[?GroupName!=`default`].GroupId' --output text)
              for SG in $SECURITY_GROUPS; do
                aws ec2 delete-security-group --group-id $SG --region ${{ env.AWS_REGION }} || echo "Failed to delete security group $SG"
              done
            else
              echo "No VPC ID found for tag 'strapi-vpc'"
            fi
          else
            echo "No VPC with tag 'strapi-vpc' found"
          fi
      - name: Import existing resources
        if: env.DESTROY == 'true'
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: |
          echo "Importing existing resources into Terraform state if not already present"
          # Check and import ECS cluster
          if aws ecs describe-clusters --clusters strapi-cluster --region ${{ env.AWS_REGION }} | grep -q '"clusterName": "strapi-cluster"'; then
            terraform state show module.ecs.aws_ecs_cluster.main >/dev/null 2>&1 || terraform import module.ecs.aws_ecs_cluster.main strapi-cluster
          else
            echo "No ECS cluster 'strapi-cluster' found, skipping import"
          fi
          # Check and import Aurora prod cluster
          if aws rds describe-db-clusters --db-cluster-identifier strapi-aurora-prod --region ${{ env.AWS_REGION }} | grep -q '"DBClusterIdentifier": "strapi-aurora-prod"'; then
            terraform state show module.aurora.aws_rds_cluster.aurora >/dev/null 2>&1 || terraform import module.aurora.aws_rds_cluster.aurora strapi-aurora-prod
          else
            echo "No Aurora prod cluster 'strapi-aurora-prod' found, skipping import"
          fi
          # Check and import Aurora staging cluster
          if aws rds describe-db-clusters --db-cluster-identifier strapi-aurora-staging --region ${{ env.AWS_REGION }} | grep -q '"DBClusterIdentifier": "strapi-aurora-staging"'; then
            terraform state show module.aurora.aws_rds_cluster.aurora >/dev/null 2>&1 || terraform import module.aurora.aws_rds_cluster.aurora strapi-aurora-staging
          else
            echo "No Aurora staging cluster 'strapi-aurora-staging' found, skipping import"
          fi
          # Check and import ALB prod
          if aws elbv2 describe-load-balancers --names strapi-prod-alb --region ${{ env.AWS_REGION }} | grep -q '"LoadBalancerName": "strapi-prod-alb"'; then
            terraform state show module.ecs.aws_lb.strapi >/dev/null 2>&1 || terraform import module.ecs.aws_lb.strapi arn:aws:elasticloadbalancing:${{ env.AWS_REGION }}:${AWS_ACCOUNT_ID}:loadbalancer/app/strapi-prod-alb/$(aws elbv2 describe-load-balancers --names strapi-prod-alb --region ${{ env.AWS_REGION }} --query 'LoadBalancers[0].LoadBalancerArn' --output text | awk -F'/' '{print $NF}')
          else
            echo "No ALB 'strapi-prod-alb' found, skipping import"
          fi
          # Check and import ALB staging
          if aws elbv2 describe-load-balancers --names strapi-staging-alb --region ${{ env.AWS_REGION }} | grep -q '"LoadBalancerName": "strapi-staging-alb"'; then
            terraform state show module.ecs.aws_lb.strapi >/dev/null 2>&1 || terraform import module.ecs.aws_lb.strapi arn:aws:elasticloadbalancing:${{ env.AWS_REGION }}:${AWS_ACCOUNT_ID}:loadbalancer/app/strapi-staging-alb/$(aws elbv2 describe-load-balancers --names strapi-staging-alb --region ${{ env.AWS_REGION }} --query 'LoadBalancers[0].LoadBalancerArn' --output text | awk -F'/' '{print $NF}')
          else
            echo "No ALB 'strapi-staging-alb' found, skipping import"
          fi
          # Check and import ECR repository
          if aws ecr describe-repositories --repository-names strapi-app --region ${{ env.AWS_REGION }} | grep -q '"repositoryName": "strapi-app"'; then
            terraform state show module.ecr.aws_ecr_repository.strapi >/dev/null 2>&1 || terraform import module.ecr.aws_ecr_repository.strapi strapi-app
          else
            echo "No ECR repository 'strapi-app' found, skipping import"
          fi
          # Check and import IAM user
          if aws iam get-user --user-name github-actions-user | grep -q '"UserName": "github-actions-user"'; then
            terraform state show module.iam.aws_iam_user.github_actions >/dev/null 2>&1 || terraform import module.iam.aws_iam_user.github_actions github-actions-user
          else
            echo "No IAM user 'github-actions-user' found, skipping import"
          fi
          # Check and import IAM user policy
          if aws iam get-user-policy --user-name github-actions-user --policy-name github-actions-policy | grep -q '"PolicyName": "github-actions-policy"'; then
            terraform state show module.iam.aws_iam_user_policy.github_actions_policy >/dev/null 2>&1 || terraform import module.iam.aws_iam_user_policy.github_actions_policy github-actions-user:github-actions-policy
          else
            echo "No IAM policy 'github_actions_policy' found, skipping import"
          fi
          # Check and import Route53 hosted zone (using domain_name variable)
          if aws route53 list-hosted-zones | grep -q "${{ secrets.DOMAIN_NAME }}"; then
            ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name=='${{ secrets.DOMAIN_NAME }}.'].Id" --output text | awk -F'/' '{print $NF}')
            if [ -n "$ZONE_ID" ]; then
              terraform state show module.route53.aws_route53_zone.main >/dev/null 2>&1 || terraform import module.route53.aws_route53_zone.main $ZONE_ID
            else
              echo "No Route53 hosted zone for ${{ secrets.DOMAIN_NAME }} found, skipping import"
            fi
          else
            echo "No Route53 hosted zones found, skipping import"
          fi
          # Check and import VPC (assuming default VPC or a known VPC ID)
          if aws ec2 describe-vpcs --filters Name=tag:Name,Values=strapi-vpc --region ${{ env.AWS_REGION }} | grep -q '"VpcId"'; then
            VPC_ID=$(aws ec2 describe-vpcs --filters Name=tag:Name,Values=strapi-vpc --region ${{ env.AWS_REGION }} --query 'Vpcs[0].VpcId' --output text)
            terraform state show module.vpc.aws_vpc.main >/dev/null 2>&1 || terraform import module.vpc.aws_vpc.main $VPC_ID
          else
            echo "No VPC with tag 'strapi-vpc' found, skipping import"
          fi
        env:
          AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}

      - name: Terraform Refresh
        if: env.DESTROY == 'true'
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: terraform refresh
        continue-on-error: true

      - name: Verify Terraform State
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: |
          echo "Checking Terraform state in S3:"
          aws s3 ls s3://${{ env.TF_STATE_BUCKET }}/${{ env.TF_STATE_KEY }} || echo "State file not found in S3"
          terraform state list || echo "Warning: No resources found in state file"

      - name: Terraform Plan
        if: env.DESTROY == 'false'
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: terraform plan -out=tfplan -var="domain_name=${{ secrets.DOMAIN_NAME }}"
        continue-on-error: false

      - name: Terraform Apply
        if: env.DESTROY == 'false'
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: terraform apply -auto-approve tfplan

      - name: Terraform Destroy
        if: env.DESTROY == 'true'
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: terraform destroy -auto-approve -var="domain_name=${{ secrets.DOMAIN_NAME }}"

      - name: Get Terraform Outputs
        if: env.DESTROY == 'false'
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        id: tf-outputs
        run: |
          echo "ECR_REPOSITORY_URL=$(terraform output -raw ecr_repository_url)" >> $GITHUB_ENV
          echo "AWS_ACCESS_KEY_ID=$(terraform output -raw aws_access_key_id)" >> $GITHUB_ENV
          echo "AWS_SECRET_ACCESS_KEY=$(terraform output -raw aws_secret_access_key)" >> $GITHUB_ENV

      - name: Update GitHub Secrets
        if: env.DESTROY == 'false'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          gh secret set AWS_ACCESS_KEY_ID -b "${{ env.AWS_ACCESS_KEY_ID }}"
          gh secret set AWS_SECRET_ACCESS_KEY -b "${{ env.AWS_SECRET_ACCESS_KEY }}"
          gh secret set ECR_REPOSITORY_URL -b "${{ env.ECR_REPOSITORY_URL }}"
        continue-on-error: true

      - name: Set up Docker Buildx
        if: env.DESTROY == 'false'
        uses: docker/setup-buildx-action@v3

      - name: Login to Amazon ECR
        if: env.DESTROY == 'false'
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2
      
      - name: Build and push Docker image
        if: env.DESTROY == 'false'
        env:
          DOCKER_IMAGE: ${{ env.ECR_REPOSITORY_URL }}:latest
        run: |
          docker build -t ${{ env.DOCKER_IMAGE }} .
          docker push ${{ env.DOCKER_IMAGE }}


          