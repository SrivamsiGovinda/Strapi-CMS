name: Deploy Strapi to AWS ECS

on:
  push:
    branches:
      - main

permissions:
  contents: read
  actions: write

env:
  AWS_REGION: us-east-1
  TERRAFORM_DIR: terraform
  TF_STATE_BUCKET: strapi-terraform-state
  TF_STATE_KEY: strapi/state/terraform.tfstate
  TF_DYNAMODB_TABLE: strapi-terraform-lock

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
      
      - name: Check commit message
        id: check-commit
        run: |
          if [[ "${{ github.event.head_commit.message }}" =~ "destroy" ]]; then
            echo "DESTROY=true" >> $GITHUB_ENV
          else
            echo "DESTROY=false" >> $GITHUB_ENV
          fi
        

      - name: Debug directory structure
        run: |
          echo "Listing repository root contents:"
          ls -la
          echo "Current working directory:"
          pwd
          echo "Listing terraform directory contents (if exists):"
          ls -la ${{ env.TERRAFORM_DIR }} || echo "No terraform directory found"
          echo "Checking for Terraform configuration files:"
          if [ -f main.tf ]; then
            echo "Found main.tf in root"
          elif [ -f ${{ env.TERRAFORM_DIR }}/main.tf ]; then
            echo "Found main.tf in terraform directory"
          else
            echo "Error: No main.tf found in root or terraform directory"
            echo "Creating minimal main.tf in terraform directory as fallback"
            mkdir -p ${{ env.TERRAFORM_DIR }}
            echo "# Minimal Terraform configuration\nterraform {\n  required_providers {\n    aws = { source = \"hashicorp/aws\", version = \"~> 4.0\" }\n  }\n  backend \"s3\" {\n    bucket         = \"${{ env.TF_STATE_BUCKET }}\"\n    key            = \"${{ env.TF_STATE_KEY }}\"\n    region         = \"${{ env.AWS_REGION }}\"\n    dynamodb_table = \"${{ env.TF_DYNAMODB_TABLE }}\"\n  }\n}\nprovider \"aws\" {\n  region = \"${{ env.AWS_REGION }}\"\n}" > ${{ env.TERRAFORM_DIR }}/main.tf
          fi
          echo "Checking for modules directory:"
          if [ -d modules ] || [ -d ${{ env.TERRAFORM_DIR }}/modules ]; then
            echo "Modules directory found"
          else
            echo "Warning: No modules directory found in root or terraform directory"
            mkdir -p ${{ env.TERRAFORM_DIR }}/modules
            echo "Created placeholder modules directory to prevent errors"
          fi

      - name: Set up Terraform directory
        run: |
          echo "Setting up Terraform directory"
          mkdir -p ${{ env.TERRAFORM_DIR }}
          if [ -f main.tf ] && [ ! -f ${{ env.TERRAFORM_DIR }}/main.tf ]; then
            mv *.tf ${{ env.TERRAFORM_DIR }}/
            if [ -d modules ]; then
              mv modules ${{ env.TERRAFORM_DIR }}/
            fi
          fi
          echo "Terraform directory contents after setup:"
          ls -la ${{ env.TERRAFORM_DIR }} || echo "Terraform directory is empty"
          if [ -f ${{ env.TERRAFORM_DIR }}/main.tf ]; then
            echo "Contents of main.tf:"
            cat ${{ env.TERRAFORM_DIR }}/main.tf
          else
            echo "Error: main.tf still not found in terraform directory"
            exit 1
          fi
          echo "Checking for local terraform.tfstate (for reference):"
          if [ -f ${{ env.TERRAFORM_DIR }}/terraform.tfstate ]; then
            echo "Found local terraform.tfstate"
            cat ${{ env.TERRAFORM_DIR }}/terraform.tfstate
          else
            echo "No local terraform.tfstate found (expected with S3 backend)"
          fi

      - name: Set Terraform working directory
        id: set-working-dir
        run: |
          if [ -f ${{ env.TERRAFORM_DIR }}/main.tf ]; then
            echo "TERRAFORM_WORKING_DIR=${{ env.TERRAFORM_DIR }}" >> $GITHUB_ENV
          else
            echo "TERRAFORM_WORKING_DIR=." >> $GITHUB_ENV
          fi

      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
     
      - name: Create S3 bucket and DynamoDB table for Terraform backend
        run: |
          echo "Creating S3 bucket '${{ env.TF_STATE_BUCKET }}' if it does not exist"
          aws s3 ls s3://${{ env.TF_STATE_BUCKET }} --region ${{ env.AWS_REGION }} >/dev/null 2>&1 || aws s3 mb s3://${{ env.TF_STATE_BUCKET }} --region ${{ env.AWS_REGION }}
          echo "Creating DynamoDB table '${{ env.TF_DYNAMODB_TABLE }}' if it does not exist"
          aws dynamodb describe-table --table-name ${{ env.TF_DYNAMODB_TABLE }} --region ${{ env.AWS_REGION }} >/dev/null 2>&1 || aws dynamodb create-table \
            --table-name ${{ env.TF_DYNAMODB_TABLE }} \
            --attribute-definitions AttributeName=LockID,AttributeType=S \
            --key-schema AttributeName=LockID,KeyType=HASH \
            --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5 \
            --region ${{ env.AWS_REGION }}
          echo "Waiting for DynamoDB table to be active"
          aws dynamodb wait table-exists --table-name ${{ env.TF_DYNAMODB_TABLE }} --region ${{ env.AWS_REGION }}
      
      - name: Validate Route53 Domain
        if: env.DESTROY == 'false'
        run: |
          if [ -z "${{ secrets.DOMAIN_NAME }}" ]; then
            echo "Error: DOMAIN_NAME secret is not set"
            exit 1
          fi
          if [[ ! "${{ secrets.DOMAIN_NAME }}" =~ ^[a-zA-Z0-9][a-zA-Z0-9.-]{1,61}[a-zA-Z0-9]\.[a-zA-Z]{2,}$ ]]; then
            echo "Error: DOMAIN_NAME '${{ secrets.DOMAIN_NAME }}' is invalid or reserved"
            exit 1
          fi
          echo "Valid domain: ${{ secrets.DOMAIN_NAME }}"

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.5.0
      
      - name: Terraform Init
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: terraform init -backend-config="bucket=${{ env.TF_STATE_BUCKET }}" -backend-config="key=${{ env.TF_STATE_KEY }}" -backend-config="region=${{ env.AWS_REGION }}" -backend-config="dynamodb_table=${{ env.TF_DYNAMODB_TABLE }}"
        continue-on-error: false

      - name: Check existing resources
        if: env.DESTROY == 'true'
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: |
          echo "Checking for existing resources in AWS:"
          aws ecs list-clusters --region ${{ env.AWS_REGION }} || echo "No ECS clusters found"
          aws rds describe-db-clusters --region ${{ env.AWS_REGION }} || echo "No RDS clusters found"
          aws elbv2 describe-load-balancers --region ${{ env.AWS_REGION }} || echo "No ALBs found"
          aws ecr describe-repositories --region ${{ env.AWS_REGION }} || echo "No ECR repositories found"
          aws route53 list-hosted-zones || echo "No Route53 hosted zones found"
          aws iam get-user --user-name github-actions-user  || echo "No IAM user 'github-actions-user ' found"
          aws iam get-user-policy --user-name github-actions-user  --policy-name github-actions-policy || echo "No IAM policy 'github-actions-policy' found for user"

      - name: Clean up dependencies
        if: env.DESTROY == 'true'
        working-directory: ${{ env.TERRAFORM_DIR }}
        run: |
          echo "Cleaning up dependencies before Terraform Destroy"
          # Delete Aurora DB instances and clusters
          for CLUSTER in strapi-aurora-prod strapi-aurora-staging; do
            if aws rds describe-db-clusters --db-cluster-identifier $CLUSTER --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
              # Delete DB instances in the cluster
              INSTANCES=$(aws rds describe-db-instances --query "DBInstances[?DBClusterIdentifier=='$CLUSTER'].DBInstanceIdentifier" --output text --region ${{ env.AWS_REGION }})
              for INSTANCE in $INSTANCES; do
                aws rds delete-db-instance --db-instance-identifier $INSTANCE --skip-final-snapshot --region ${{ env.AWS_REGION }} || echo "Failed to delete DB instance $INSTANCE"
                aws rds wait db-instance-deleted --db-instance-identifier $INSTANCE --region ${{ env.AWS_REGION }} || echo "Timeout waiting for DB instance $INSTANCE deletion"
              done
              # Delete the DB cluster
              aws rds delete-db-cluster --db-cluster-identifier $CLUSTER --skip-final-snapshot --region ${{ env.AWS_REGION }} || echo "Failed to delete DB cluster $CLUSTER"
              aws rds wait db-cluster-deleted --db-cluster-identifier $CLUSTER --region ${{ env.AWS_REGION }} || echo "Timeout waiting for DB cluster $CLUSTER deletion"
            else
              echo "No Aurora DB cluster '$CLUSTER' found, skipping cleanup"
            fi
            # Delete DB subnet group
            SUBNET_GROUP=strapi-aurora-subnet-group
            if aws rds describe-db-subnet-groups --db-subnet-group-name $SUBNET_GROUP --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
              aws rds delete-db-subnet-group --db-subnet-group-name $SUBNET_GROUP --region ${{ env.AWS_REGION }} || echo "Failed to delete DB subnet group $SUBNET_GROUP"
            else
              echo "No DB subnet group '$SUBNET_GROUP' found, skipping cleanup"
            fi
          done
          # Delete Secrets Manager secrets
          for SECRET in strapi-prod-db-password strapi-staging-db-password; do
            if aws secretsmanager describe-secret --secret-id $SECRET --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
              aws secretsmanager delete-secret --secret-id $SECRET --force-delete-without-recovery --region ${{ env.AWS_REGION }} || echo "Failed to delete secret $SECRET"
            else
              echo "No secret '$SECRET' found, skipping cleanup"
            fi
          done
          
          # Delete IAM role only if DESTROY=true
          if [ "${{ env.DESTROY }}" = "true" ]; then
            if aws iam get-role --role-name strapi-ecs-execution-role >/dev/null 2>&1; then
              # Detach inline policies
              INLINE_POLICIES=$(aws iam list-role-policies --role-name strapi-ecs-execution-role --query 'PolicyNames[]' --output text --region ${{ env.AWS_REGION }})
              for POLICY in $INLINE_POLICIES; do
                aws iam delete-role-policy --role-name strapi-ecs-execution-role --policy-name $POLICY --region ${{ env.AWS_REGION }} || echo "Failed to delete inline policy $POLICY from strapi-ecs-execution-role"
              done
              # Detach managed policies
              MANAGED_POLICIES=$(aws iam list-attached-role-policies --role-name strapi-ecs-execution-role --query 'AttachedPolicies[].PolicyArn' --output text --region ${{ env.AWS_REGION }})
              for POLICY in $MANAGED_POLICIES; do
                aws iam detach-role-policy --role-name strapi-ecs-execution-role --policy-arn $POLICY --region ${{ env.AWS_REGION }} || echo "Failed to detach managed policy $POLICY from strapi-ecs-execution-role"
              done
              aws iam delete-role --role-name strapi-ecs-execution-role --region ${{ env.AWS_REGION }} || echo "Failed to delete IAM role strapi-ecs-execution-role"
            else
              echo "No IAM role 'strapi-ecs-execution-role' found, skipping cleanup"
            fi
          else
            echo "DESTROY=false, skipping IAM role 'strapi-ecs-execution-role' cleanup"
          fi

          # Delete ELBv2 target groups
          for TG in strapi-prod-tg strapi-staging-tg; do
            if aws elbv2 describe-target-groups --names $TG --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
              TG_ARN=$(aws elbv2 describe-target-groups --names $TG --region ${{ env.AWS_REGION }} --query 'TargetGroups[0].TargetGroupArn' --output text)
              aws elbv2 delete-target-group --target-group-arn $TG_ARN --region ${{ env.AWS_REGION }} || echo "Failed to delete target group $TG"
            else
              echo "No target group '$TG' found, skipping cleanup"
            fi
          done
          # Delete CloudWatch log groups
          for LOG_GROUP in /ecs/strapi-prod /ecs/strapi-staging; do
            if aws logs describe-log-groups --log-group-name-prefix $LOG_GROUP --region ${{ env.AWS_REGION }} | grep -q "\"logGroupName\": \"$LOG_GROUP\""; then
              aws logs delete-log-group --log-group-name $LOG_GROUP --region ${{ env.AWS_REGION }} || echo "Failed to delete log group $LOG_GROUP"
            else
              echo "No log group '$LOG_GROUP' found, skipping cleanup"
            fi
          done
          # Scale down ECS services to zero
          if aws ecs list-services --cluster strapi-cluster --region ${{ env.AWS_REGION }} | grep -q 'strapi-prod-service'; then
            aws ecs update-service --cluster strapi-cluster --service strapi-prod-service --desired-count 0 --region ${{ env.AWS_REGION }}
            aws ecs delete-service --cluster strapi-cluster --service strapi-prod-service --region ${{ env.AWS_REGION }}
          else
            echo "No ECS service 'strapi-prod-service' found"
          fi
          if aws ecs list-services --cluster strapi-cluster --region ${{ env.AWS_REGION }} | grep -q 'strapi-staging-service'; then
            aws ecs update-service --cluster strapi-cluster --service strapi-staging-service --desired-count 0 --region ${{ env.AWS_REGION }}
            aws ecs delete-service --cluster strapi-cluster --service strapi-staging-service --region ${{ env.AWS_REGION }}
          else
            echo "No ECS service 'strapi-staging-service' found"
          fi
          # Delete ECS tasks to release ENIs
          if aws ecs list-tasks --cluster strapi-cluster --region ${{ env.AWS_REGION }} | grep -q '"taskArn"'; then
            TASK_ARNS=$(aws ecs list-tasks --cluster strapi-cluster --region ${{ env.AWS_REGION }} --query 'taskArns[]' --output text)
            for TASK in $TASK_ARNS; do
              aws ecs stop-task --cluster strapi-cluster --task $TASK --region ${{ env.AWS_REGION }} || echo "Failed to stop task $TASK"
            done
          else
            echo "No ECS tasks found in cluster 'strapi-cluster'"
          fi
          # Delete ALBs to release ENIs
          for ALB in strapi-prod-alb strapi-staging-alb; do
            if aws elbv2 describe-load-balancers --names $ALB --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
              ALB_ARN=$(aws elbv2 describe-load-balancers --names $ALB --region ${{ env.AWS_REGION }} --query 'LoadBalancers[0].LoadBalancerArn' --output text)
              aws elbv2 delete-load-balancer --load-balancer-arn $ALB_ARN --region ${{ env.AWS_REGION }} || echo "Failed to delete ALB $ALB"
              aws elbv2 wait load-balancers-deleted --load-balancer-arns $ALB_ARN --region ${{ env.AWS_REGION }}
            else
              echo "No ALB '$ALB' found"
            fi
          done
          # Delete ECR images (skip if repository doesn't exist)
          if aws ecr describe-repositories --repository-names strapi-app --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            IMAGE_DIGESTS=$(aws ecr list-images --repository-name strapi-app --region ${{ env.AWS_REGION }} --query 'imageIds[].imageDigest' --output text)
            if [ -n "$IMAGE_DIGESTS" ]; then
              for DIGEST in $IMAGE_DIGESTS; do
                aws ecr batch-delete-image --repository-name strapi-app --image-ids imageDigest=$DIGEST --region ${{ env.AWS_REGION }}
              done
            else
              echo "No images found in ECR repository 'strapi-app'"
            fi
          else
            echo "No ECR repository 'strapi-app' found, skipping cleanup"
          fi
          # Delete IAM access keys (skip if user doesn't exist)
          if aws iam get-user --user-name github-actions-user >/dev/null 2>&1; then
            ACCESS_KEYS=$(aws iam list-access-keys --user-name github-actions-user --query 'AccessKeyMetadata[].AccessKeyId' --output text)
            if [ -n "$ACCESS_KEYS" ]; then
              for KEY in $ACCESS_KEYS; do
                aws iam delete-access-key --user-name github-actions-user --access-key-id $KEY
              done
            else
              echo "No access keys found for IAM user 'github-actions-user'"
            fi
          else
            echo "No IAM user 'github-actions-user' found, skipping cleanup"
          fi
          # Clean up VPC dependencies for all VPCs
          VPC_IDS=$(aws ec2 describe-vpcs --region ${{ env.AWS_REGION }} --query 'Vpcs[?IsDefault==`false`].VpcId' --output text)
          if [ -n "$VPC_IDS" ]; then
            for VPC_ID in $VPC_IDS; do
              echo "Cleaning up VPC $VPC_ID"
              # Delete ENIs
              ENIS=$(aws ec2 describe-network-interfaces --filters Name=vpc-id,Values=$VPC_ID --region ${{ env.AWS_REGION }} --query 'NetworkInterfaces[].NetworkInterfaceId' --output text)
              for ENI in $ENIS; do
                aws ec2 delete-network-interface --network-interface-id $ENI --region ${{ env.AWS_REGION }} || echo "Failed to delete ENI $ENI in VPC $VPC_ID"
              done
              # Delete subnets
              SUBNETS=$(aws ec2 describe-subnets --filters Name=vpc-id,Values=$VPC_ID --region ${{ env.AWS_REGION }} --query 'Subnets[].SubnetId' --output text)
              for SUBNET in $SUBNETS; do
                aws ec2 delete-subnet --subnet-id $SUBNET --region ${{ env.AWS_REGION }} || echo "Failed to delete subnet $SUBNET in VPC $VPC_ID"
              done
              # Delete routes and disassociate route tables (except main)
              ROUTE_TABLES=$(aws ec2 describe-route-tables --filters Name=vpc-id,Values=$VPC_ID --region ${{ env.AWS_REGION }} --query 'RouteTables[].RouteTableId' --output text)
              for RT in $ROUTE_TABLES; do
                # Delete routes pointing to internet gateway
                IGW=$(aws ec2 describe-internet-gateways --filters Name=attachment.vpc-id,Values=$VPC_ID --region ${{ env.AWS_REGION }} --query 'InternetGateways[].InternetGatewayId' --output text)
                if [ -n "$IGW" ]; then
                  ROUTES=$(aws ec2 describe-route-tables --route-table-ids $RT --region ${{ env.AWS_REGION }} --query 'RouteTables[0].Routes[?GatewayId==`'$IGW'`].DestinationCidrBlock' --output text)
                  for CIDR in $ROUTES; do
                    aws ec2 delete-route --route-table-id $RT --destination-cidr-block $CIDR --region ${{ env.AWS_REGION }} || echo "Failed to delete route $CIDR in $RT for VPC $VPC_ID"
                  done
                fi
                # Disassociate route table
                ASSOCIATIONS=$(aws ec2 describe-route-tables --route-table-ids $RT --region ${{ env.AWS_REGION }} --query 'RouteTables[0].Associations[].RouteTableAssociationId' --output text)
                for ASSOC in $ASSOCIATIONS; do
                  aws ec2 disassociate-route-table --association-id $ASSOC --region ${{ env.AWS_REGION }} || echo "Failed to disassociate route table $RT in VPC $VPC_ID"
                done
                aws ec2 delete-route-table --route-table-id $RT --region ${{ env.AWS_REGION }} || echo "Failed to delete route table $RT in VPC $VPC_ID"
              done
              # Delete internet gateway
              if [ -n "$IGW" ]; then
                aws ec2 detach-internet-gateway --internet-gateway-id $IGW --vpc-id $VPC_ID --region ${{ env.AWS_REGION }} || echo "Failed to detach internet gateway $IGW in VPC $VPC_ID"
                aws ec2 delete-internet-gateway --internet-gateway-id $IGW --region ${{ env.AWS_REGION }} || echo "Failed to delete internet gateway $IGW in VPC $VPC_ID"
              fi
              # Delete security groups (except default)
              SECURITY_GROUPS=$(aws ec2 describe-security-groups --filters Name=vpc-id,Values=$VPC_ID --region ${{ env.AWS_REGION }} --query 'SecurityGroups[?GroupName!=`default`].GroupId' --output text)
              for SG in $SECURITY_GROUPS; do
                aws ec2 delete-security-group --group-id $SG --region ${{ env.AWS_REGION }} || echo "Failed to delete security group $SG in VPC $VPC_ID"
              done
            done
          else
            echo "No VPCs found in region ${{ env.AWS_REGION }}"
          fi
      
      - name: Import existing resources
        if: env.DESTROY == 'true'
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: |
          echo "Importing existing resources into Terraform state if not already present"
          # Check and import ECS cluster
          echo "Checking for ECS cluster 'strapi-cluster':"
          CLUSTER_CHECK=$(aws ecs describe-clusters --clusters strapi-cluster --region ${{ env.AWS_REGION }} --query 'clusters[?clusterName==`strapi-cluster` && status!=`INACTIVE`]' --output json)
          echo "ECS cluster check response: $CLUSTER_CHECK"
          if [ -n "$CLUSTER_CHECK" ] && [ "$CLUSTER_CHECK" != "[]" ] && echo "$CLUSTER_CHECK" | grep -q '"clusterName": "strapi-cluster"'; then
            terraform state show module.ecs.aws_ecs_cluster.main >/dev/null 2>&1 || terraform import module.ecs.aws_ecs_cluster.main strapi-cluster
          else
            echo "No active ECS cluster 'strapi-cluster' found, skipping import"
          fi
          # Check and import Aurora prod cluster
          if aws rds describe-db-clusters --db-cluster-identifier strapi-aurora-prod --region ${{ env.AWS_REGION }} | grep -q '"DBClusterIdentifier": "strapi-aurora-prod"'; then
            terraform state show module.aurora.aws_rds_cluster.aurora >/dev/null 2>&1 || terraform import module.aurora.aws_rds_cluster.aurora strapi-aurora-prod
          else
            echo "No Aurora prod cluster 'strapi-aurora-prod' found, skipping import"
          fi
          # Check and import Aurora staging cluster
          if aws rds describe-db-clusters --db-cluster-identifier strapi-aurora-staging --region ${{ env.AWS_REGION }} | grep -q '"DBClusterIdentifier": "strapi-aurora-staging"'; then
            terraform state show module.aurora.aws_rds_cluster.aurora >/dev/null 2>&1 || terraform import module.aurora.aws_rds_cluster.aurora strapi-aurora-staging
          else
            echo "No Aurora staging cluster 'strapi-aurora-staging' found, skipping import"
          fi
          # Check and import ALB prod
          if aws elbv2 describe-load-balancers --names strapi-prod-alb --region ${{ env.AWS_REGION }} | grep -q '"LoadBalancerName": "strapi-prod-alb"'; then
            terraform state show module.ecs.aws_lb.strapi >/dev/null 2>&1 || terraform import module.ecs.aws_lb.strapi arn:aws:elasticloadbalancing:${{ env.AWS_REGION }}:${AWS_ACCOUNT_ID}:loadbalancer/app/strapi-prod-alb/$(aws elbv2 describe-load-balancers --names strapi-prod-alb --region ${{ env.AWS_REGION }} --query 'LoadBalancers[0].LoadBalancerArn' --output text | awk -F'/' '{print $NF}')
          else
            echo "No ALB 'strapi-prod-alb' found, skipping import"
          fi
          # Check and import ALB staging
          if aws elbv2 describe-load-balancers --names strapi-staging-alb --region ${{ env.AWS_REGION }} | grep -q '"LoadBalancerName": "strapi-staging-alb"'; then
            terraform state show module.ecs.aws_lb.strapi >/dev/null 2>&1 || terraform import module.ecs.aws_lb.strapi arn:aws:elasticloadbalancing:${{ env.AWS_REGION }}:${AWS_ACCOUNT_ID}:loadbalancer/app/strapi-staging-alb/$(aws elbv2 describe-load-balancers --names strapi-staging-alb --region ${{ env.AWS_REGION }} --query 'LoadBalancers[0].LoadBalancerArn' --output text | awk -F'/' '{print $NF}')
          else
            echo "No ALB 'strapi-staging-alb' found, skipping import"
          fi
          # Check and import ECR repository
          if aws ecr describe-repositories --repository-names strapi-app --region ${{ env.AWS_REGION }} | grep -q '"repositoryName": "strapi-app"'; then
            terraform state show module.ecr.aws_ecr_repository.strapi >/dev/null 2>&1 || terraform import module.ecr.aws_ecr_repository.strapi strapi-app
          else
            echo "No ECR repository 'strapi-app' found, skipping import"
          fi
          # Check and import IAM user
          if aws iam get-user --user-name github-actions-user | grep -q '"UserName": "github-actions-user"'; then
            terraform state show module.iam.aws_iam_user.github_actions >/dev/null 2>&1 || terraform import module.iam.aws_iam_user.github_actions github-actions-user
          else
            echo "No IAM user 'github-actions-user' found, skipping import"
          fi
          # Check and import IAM user policy
          if aws iam get-user-policy --user-name github-actions-user --policy-name github-actions-policy | grep -q '"PolicyName": "github-actions-policy"'; then
            terraform state show module.iam.aws_iam_user_policy.github_actions_policy >/dev/null 2>&1 || terraform import module.iam.aws_iam_user_policy.github_actions_policy github-actions-user:github-actions-policy
          else
            echo "No IAM policy 'github_actions_policy' found, skipping import"
          fi
          # Check and import Route53 hosted zone (using domain_name variable)
          if aws route53 list-hosted-zones | grep -q "${{ secrets.DOMAIN_NAME }}"; then
            ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name=='${{ secrets.DOMAIN_NAME }}.'].Id" --output text | awk -F'/' '{print $NF}')
            if [ -n "$ZONE_ID" ]; then
              terraform state show module.route53.aws_route53_zone.main >/dev/null 2>&1 || terraform import module.route53.aws_route53_zone.main $ZONE_ID
            else
              echo "No Route53 hosted zone for ${{ secrets.DOMAIN_NAME }} found, skipping import"
            fi
          else
            echo "No Route53 hosted zones found, skipping import"
          fi
          # Check and import non-default VPCs
          VPC_IDS=$(aws ec2 describe-vpcs --region ${{ env.AWS_REGION }} --query 'Vpcs[?IsDefault==`false`].VpcId' --output text)
          if [ -n "$VPC_IDS" ]; then
            for VPC_ID in $VPC_IDS; do
              echo "Checking non-default VPC $VPC_ID"
              echo "Importing VPC $VPC_ID into module.vpc.aws_vpc.main"
              terraform state show module.vpc.aws_vpc.main >/dev/null 2>&1 || terraform import module.vpc.aws_vpc.main $VPC_ID
            done
          else
              echo "No non-default VPCs found in region ${{ env.AWS_REGION }}, skipping import"
          fi
        env:
          AWS_ACCOUNT_ID: ${{ secrets.AWS_ACCOUNT_ID }}

      - name: Terraform Refresh
        if: env.DESTROY == 'true'
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: terraform refresh
        continue-on-error: true

      - name: Verify Terraform State
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: |
          echo "Checking Terraform state in S3:"
          aws s3 ls s3://${{ env.TF_STATE_BUCKET }}/${{ env.TF_STATE_KEY }} || echo "State file not found in S3"
          terraform state list || echo "Warning: No resources found in state file"

      - name: Terraform Plan
        if: env.DESTROY == 'false'
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: terraform plan -out=tfplan -var="domain_name=${{ secrets.DOMAIN_NAME }}"
        continue-on-error: false

      - name: Terraform Apply
        if: env.DESTROY == 'false'
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: terraform apply -auto-approve tfplan

      - name: Terraform Destroy
        if: env.DESTROY == 'true'
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        run: terraform destroy -auto-approve -var="domain_name=${{ secrets.DOMAIN_NAME }}"

      - name: Get Terraform Outputs
        if: env.DESTROY == 'false'
        working-directory: ${{ env.TERRAFORM_WORKING_DIR }}
        id: tf-outputs
        run: |
          echo "ECR_REPOSITORY_URL=$(terraform output -raw ecr_repository_url)" >> $GITHUB_ENV
          echo "AWS_ACCESS_KEY_ID=$(terraform output -raw aws_access_key_id)" >> $GITHUB_ENV
          echo "AWS_SECRET_ACCESS_KEY=$(terraform output -raw aws_secret_access_key)" >> $GITHUB_ENV

      - name: Update GitHub Secrets
        if: env.DESTROY == 'false'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          gh secret set AWS_ACCESS_KEY_ID -b "${{ env.AWS_ACCESS_KEY_ID }}"
          gh secret set AWS_SECRET_ACCESS_KEY -b "${{ env.AWS_SECRET_ACCESS_KEY }}"
          gh secret set ECR_REPOSITORY_URL -b "${{ env.ECR_REPOSITORY_URL }}"
        continue-on-error: true

      - name: Set up Docker Buildx
        if: env.DESTROY == 'false'
        uses: docker/setup-buildx-action@v3

      - name: Login to Amazon ECR
        if: env.DESTROY == 'false'
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2
      
      - name: Build and push Docker image
        if: env.DESTROY == 'false'
        env:
          DOCKER_IMAGE: ${{ env.ECR_REPOSITORY_URL }}:latest
        run: |
           docker build --no-cache -t ${{ env.DOCKER_IMAGE }} .
           docker push ${{ env.DOCKER_IMAGE }}



        
          
